<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Estimations | CM 2110 Calculus and Statistical Distributions</title>
  <meta name="description" content="Chapter 2 Estimations | CM 2110 Calculus and Statistical Distributions" />
  <meta name="generator" content="bookdown 0.18.3 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Estimations | CM 2110 Calculus and Statistical Distributions" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Estimations | CM 2110 Calculus and Statistical Distributions" />
  
  
  

<meta name="author" content="Dr. Priyanga D. Talagala" />


<meta name="date" content="2020-08-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistical-distributions.html"/>
<link rel="next" href="hypothesis-testing.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">CM 1110 Fundamentals of Mathematics and Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course Syllabus</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#pre-requisites"><i class="fa fa-check"></i>Pre-requisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#outline-syllabus"><i class="fa fa-check"></i>Outline Syllabus</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#method-of-assessment"><i class="fa fa-check"></i>Method of Assessment</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recommended-texts"><i class="fa fa-check"></i>Recommended Texts</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#lecturer"><i class="fa fa-check"></i>Lecturer</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#schedule"><i class="fa fa-check"></i>Schedule</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="statistical-distributions.html"><a href="statistical-distributions.html"><i class="fa fa-check"></i><b>1</b> Statistical Distributions</a><ul>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#recap-cm-1110-probability"><i class="fa fa-check"></i>Recap: CM 1110-Probability</a><ul>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#axioms-of-probability"><i class="fa fa-check"></i>Axioms of probability</a></li>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#methods-for-determining-probability"><i class="fa fa-check"></i>Methods for determining Probability</a></li>
</ul></li>
<li class="chapter" data-level="1.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#random-variable"><i class="fa fa-check"></i><b>1.1</b> Random Variable</a><ul>
<li class="chapter" data-level="1.1.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#types-of-random-variables"><i class="fa fa-check"></i><b>1.1.1</b> Types of Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#probability-mass-function"><i class="fa fa-check"></i><b>1.2</b> Probability Mass Function</a><ul>
<li class="chapter" data-level="1.2.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#properties-of-a-probability-mass-function"><i class="fa fa-check"></i><b>1.2.1</b> Properties of a Probability Mass Function</a></li>
<li class="chapter" data-level="1.2.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#representations-of-probability-mass-functions"><i class="fa fa-check"></i><b>1.2.2</b> Representations of Probability Mass Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="statistical-distributions.html"><a href="statistical-distributions.html#probability-density-function"><i class="fa fa-check"></i><b>1.3</b> Probability Density Function</a><ul>
<li class="chapter" data-level="1.3.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#properties-of-a-probability-density-function"><i class="fa fa-check"></i><b>1.3.1</b> Properties of a Probability Density Function</a></li>
<li class="chapter" data-level="1.3.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#existence-of-pdf"><i class="fa fa-check"></i><b>1.3.2</b> Existence of pdf</a></li>
<li class="chapter" data-level="1.3.3" data-path="statistical-distributions.html"><a href="statistical-distributions.html#calculation-of-probability-using-pdf"><i class="fa fa-check"></i><b>1.3.3</b> Calculation of Probability using pdf</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="statistical-distributions.html"><a href="statistical-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>1.4</b> Cumulative Distribution Function</a><ul>
<li class="chapter" data-level="1.4.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#relationship-between-cdf-and-pdf"><i class="fa fa-check"></i><b>1.4.1</b> Relationship between cdf and pdf</a></li>
<li class="chapter" data-level="1.4.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#properties-of-a-cumulative-distribution-function-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>1.4.2</b> Properties of a cumulative distribution function of a Discrete random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="statistical-distributions.html"><a href="statistical-distributions.html#expectations-and-moments"><i class="fa fa-check"></i><b>1.5</b> Expectations and Moments</a><ul>
<li class="chapter" data-level="1.5.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#expectation"><i class="fa fa-check"></i><b>1.5.1</b> Expectation</a></li>
<li class="chapter" data-level="1.5.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#moments"><i class="fa fa-check"></i><b>1.5.2</b> Moments</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="statistical-distributions.html"><a href="statistical-distributions.html#models-for-discrete-distributions"><i class="fa fa-check"></i><b>1.6</b> Models for Discrete Distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#discrete-uniform-distribution"><i class="fa fa-check"></i><b>1.6.1</b> Discrete Uniform Distribution</a></li>
<li class="chapter" data-level="1.6.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#bernoulli-distribution"><i class="fa fa-check"></i><b>1.6.2</b> Bernoulli Distribution</a></li>
<li class="chapter" data-level="1.6.3" data-path="statistical-distributions.html"><a href="statistical-distributions.html#binomial-distribution"><i class="fa fa-check"></i><b>1.6.3</b> Binomial Distribution</a></li>
<li class="chapter" data-level="1.6.4" data-path="statistical-distributions.html"><a href="statistical-distributions.html#geometric-distribution"><i class="fa fa-check"></i><b>1.6.4</b> Geometric Distribution</a></li>
<li class="chapter" data-level="1.6.5" data-path="statistical-distributions.html"><a href="statistical-distributions.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>1.6.5</b> Negative Binomial Distribution</a></li>
<li class="chapter" data-level="1.6.6" data-path="statistical-distributions.html"><a href="statistical-distributions.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>1.6.6</b> Hypergeometric Distribution</a></li>
<li class="chapter" data-level="1.6.7" data-path="statistical-distributions.html"><a href="statistical-distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>1.6.7</b> Poisson Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="statistical-distributions.html"><a href="statistical-distributions.html#models-for-continuous-distributions"><i class="fa fa-check"></i><b>1.7</b> Models for Continuous Distributions</a><ul>
<li class="chapter" data-level="1.7.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>1.7.1</b> Uniform Distribution</a></li>
<li class="chapter" data-level="1.7.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#normal-distribution-gaussian-distribution"><i class="fa fa-check"></i><b>1.7.2</b> Normal Distribution (Gaussian Distribution)</a></li>
<li class="chapter" data-level="1.7.3" data-path="statistical-distributions.html"><a href="statistical-distributions.html#gamma-distribution"><i class="fa fa-check"></i><b>1.7.3</b> Gamma Distribution</a></li>
<li class="chapter" data-level="1.7.4" data-path="statistical-distributions.html"><a href="statistical-distributions.html#exponential-distribution"><i class="fa fa-check"></i><b>1.7.4</b> Exponential Distribution</a></li>
<li class="chapter" data-level="1.7.5" data-path="statistical-distributions.html"><a href="statistical-distributions.html#beta-distribution"><i class="fa fa-check"></i><b>1.7.5</b> Beta Distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="statistical-distributions.html"><a href="statistical-distributions.html#approximations"><i class="fa fa-check"></i><b>1.8</b> Approximations</a><ul>
<li class="chapter" data-level="1.8.1" data-path="statistical-distributions.html"><a href="statistical-distributions.html#poisson-approximation-to-binomial"><i class="fa fa-check"></i><b>1.8.1</b> Poisson approximation to Binomial</a></li>
<li class="chapter" data-level="1.8.2" data-path="statistical-distributions.html"><a href="statistical-distributions.html#normal-approximation-to-binomial"><i class="fa fa-check"></i><b>1.8.2</b> Normal approximation to Binomial</a></li>
<li class="chapter" data-level="1.8.3" data-path="statistical-distributions.html"><a href="statistical-distributions.html#normal-approximation-to-poisson"><i class="fa fa-check"></i><b>1.8.3</b> Normal approximation to Poisson</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="statistical-distributions.html"><a href="statistical-distributions.html#distribution-of-functions-of-random-variables"><i class="fa fa-check"></i><b>1.9</b> Distribution of Functions of Random Variables</a></li>
<li class="chapter" data-level="1.10" data-path="statistical-distributions.html"><a href="statistical-distributions.html#distribution-of-sum-of-independent-random-variables"><i class="fa fa-check"></i><b>1.10</b> Distribution of Sum of Independent Random Variables</a></li>
<li class="chapter" data-level="1.11" data-path="statistical-distributions.html"><a href="statistical-distributions.html#sampling-distribution"><i class="fa fa-check"></i><b>1.11</b> Sampling Distribution</a></li>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#references"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#tutorial"><i class="fa fa-check"></i>Tutorial</a></li>
<li class="chapter" data-level="" data-path="statistical-distributions.html"><a href="statistical-distributions.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="estimations.html"><a href="estimations.html"><i class="fa fa-check"></i><b>2</b> Estimations</a><ul>
<li class="chapter" data-level="2.1" data-path="estimations.html"><a href="estimations.html#statistical-inference"><i class="fa fa-check"></i><b>2.1</b> Statistical Inference</a></li>
<li class="chapter" data-level="2.2" data-path="estimations.html"><a href="estimations.html#point-estimation"><i class="fa fa-check"></i><b>2.2</b> Point Estimation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="estimations.html"><a href="estimations.html#methods-of-finding-point-estimators"><i class="fa fa-check"></i><b>2.2.1</b> Methods of finding point estimators</a></li>
<li class="chapter" data-level="2.2.2" data-path="estimations.html"><a href="estimations.html#desirable-properties-of-point-estimators"><i class="fa fa-check"></i><b>2.2.2</b> Desirable properties of point estimators</a></li>
<li class="chapter" data-level="2.2.3" data-path="estimations.html"><a href="estimations.html#consistency"><i class="fa fa-check"></i><b>2.2.3</b> Consistency</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="estimations.html"><a href="estimations.html#interval-estimation"><i class="fa fa-check"></i><b>2.3</b> Interval Estimation</a><ul>
<li class="chapter" data-level="2.3.1" data-path="estimations.html"><a href="estimations.html#what-is-gained-by-using-an-interval-estimator"><i class="fa fa-check"></i><b>2.3.1</b> What is gained by using an Interval Estimator?</a></li>
<li class="chapter" data-level="2.3.2" data-path="estimations.html"><a href="estimations.html#definition-of-confidence-interval"><i class="fa fa-check"></i><b>2.3.2</b> Definition of confidence interval</a></li>
<li class="chapter" data-level="2.3.3" data-path="estimations.html"><a href="estimations.html#interpretation-of-confidence-intervals"><i class="fa fa-check"></i><b>2.3.3</b> Interpretation of confidence intervals</a></li>
<li class="chapter" data-level="2.3.4" data-path="estimations.html"><a href="estimations.html#methods-of-finding-interval-estimators"><i class="fa fa-check"></i><b>2.3.4</b> Methods of finding interval estimators</a></li>
<li class="chapter" data-level="2.3.5" data-path="estimations.html"><a href="estimations.html#methods-of-evaluating-interval-estimators"><i class="fa fa-check"></i><b>2.3.5</b> Methods of evaluating interval estimators</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="estimations.html"><a href="estimations.html#tutorial-1"><i class="fa fa-check"></i>Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>3</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#null-and-alternative-hypotheses"><i class="fa fa-check"></i><b>3.1</b> Null and alternative hypotheses</a></li>
<li class="chapter" data-level="3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#errors-in-testing-hypotheses-type-i-and-type-ii-error"><i class="fa fa-check"></i><b>3.2</b> Errors in testing hypotheses-type I and type II error</a></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#significance-level-size-power-of-a-test"><i class="fa fa-check"></i><b>3.3</b> Significance level, size, power of a test</a></li>
<li class="chapter" data-level="3.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#formulation-of-hypotheses"><i class="fa fa-check"></i><b>3.4</b> Formulation of hypotheses</a></li>
<li class="chapter" data-level="3.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#methods-of-testing-hypotheses"><i class="fa fa-check"></i><b>3.5</b> Methods of testing hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="design-of-experiments.html"><a href="design-of-experiments.html"><i class="fa fa-check"></i><b>4</b> Design of Experiments</a><ul>
<li class="chapter" data-level="4.1" data-path="design-of-experiments.html"><a href="design-of-experiments.html#ntroduction-to-experimental-design"><i class="fa fa-check"></i><b>4.1</b> ntroduction to experimental design</a></li>
<li class="chapter" data-level="4.2" data-path="design-of-experiments.html"><a href="design-of-experiments.html#basic-principles-of-experimental-design"><i class="fa fa-check"></i><b>4.2</b> Basic principles of experimental design</a></li>
<li class="chapter" data-level="4.3" data-path="design-of-experiments.html"><a href="design-of-experiments.html#completely-randomized-design"><i class="fa fa-check"></i><b>4.3</b> Completely randomized design</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">CM 2110 Calculus and Statistical Distributions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimations" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Estimations</h1>

<div id="statistical-inference" class="section level2">
<h2><span class="header-section-number">2.1</span> Statistical Inference</h2>
<ul>
<li>The process of making educated guess and conclusions regarding a population, using a sample of that population is called <strong>Statistical Inference</strong>.</li>
<li>Two important problems in statistical inference are <strong>estimation of parameters</strong> and <strong>tests of hypothesis</strong></li>
<li>Estimation can be of the form of <strong>point estimation</strong> and <strong>interval estimation</strong>.</li>
</ul>
<p><img src="figure/Ch2box1-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="point-estimation" class="section level2">
<h2><span class="header-section-number">2.2</span> Point Estimation</h2>
<p><strong>Main Task</strong></p>
<ul>
<li>Assume that some characteristic of the elements in a population can be represented by a random variable <span class="math inline">\(X\)</span>.</li>
<li>Assume that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> is a random sample from a density <span class="math inline">\(f(x, \theta)\)</span>, where the form of the density is known but the parameter <span class="math inline">\(\theta\)</span> is unknown.</li>
<li>The objective is to construct good estimators for <span class="math inline">\(\theta\)</span> or its function <span class="math inline">\(\tau (\theta)\)</span> on the basis of the observed sample values <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> of a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from <span class="math inline">\(f(x, \theta)\)</span>.</li>
</ul>
<p><strong>Definition: Statistic</strong></p>
<p>Suppose <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be <span class="math inline">\(n\)</span> observable random variables. Then, a known function <span class="math inline">\(T=g(X_1, X_2, \dots, X_n)\)</span> of observable random variables <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> is called a <strong>statistic</strong>. A statistic is always a random variable.</p>
<p><strong>Definition: Estimator</strong></p>
<p>Suppose <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> is a random sample from from a density <span class="math inline">\(f(x, \theta)\)</span> and it is desired to estimate <span class="math inline">\(\theta\)</span>. Suppose <span class="math inline">\(T=g(X_1, X_2, \dots, X_n)\)</span> is a <em>statistic</em> that can be used to determine and approximate value for <span class="math inline">\(\theta\)</span>. Then <span class="math inline">\(T\)</span> is called an <strong>estimator</strong> for <span class="math inline">\(\theta\)</span>. An estimator is always a random variable.</p>
<p><strong>Definition: Estimate</strong></p>
<p>Suppose <span class="math inline">\(T=g(X_1, X_2, \dots, X_n)\)</span> be an estimator for <span class="math inline">\(\theta\)</span>. Suppose that <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> is a set of observed values of the random variable <span class="math inline">\(X_1, X_2, \dots, X_n.\)</span> Then <em>the value</em> <span class="math inline">\(t=g(x_1, x_2, \dots, x_n)\)</span> obtained by substituting the observed values in the estimator is called an <strong>estimate</strong> for <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li><p>Therefore the <strong>estimator</strong> stands for the function of the sample, and the word <strong>estimate</strong> stands for the realized value of that function.</p></li>
<li><p><em>Notation:</em> An estimator of <span class="math inline">\(\theta\)</span> is denoted by <span class="math inline">\(\hat{\theta}\)</span>. An estimate of <span class="math inline">\(\theta\)</span> is also denoted by <span class="math inline">\(\hat{\theta}\)</span>. The difference between the two should be understood based on the context.</p></li>
</ul>
<table>
<colgroup>
<col width="23%" />
<col width="25%" />
<col width="27%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Estimator: Using random sample (<span class="math inline">\(X_1, X_2, \dots, X_n\)</span>)</th>
<th>Estimate 1: Using observed sample (<span class="math inline">\(1,4,2,3,4\)</span>)</th>
<th>Estimate 2: Using observed sample (<span class="math inline">\(4,2,2,6,3\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mu\)</span></td>
<td><span class="math inline">\(\hat{\mu}=\bar{X}\)</span></td>
<td><span class="math inline">\(\hat{\mu}=\)</span></td>
<td><span class="math inline">\(\hat{\mu}=\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\sigma^2\)</span></td>
<td><span class="math inline">\(\hat{\sigma^2}=S^2\)</span></td>
<td><span class="math inline">\(\hat{\sigma^2}=\)</span></td>
<td><span class="math inline">\(\hat{\sigma^2}=\)</span></td>
</tr>
</tbody>
</table>
<div id="methods-of-finding-point-estimators" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Methods of finding point estimators</h3>
<ul>
<li>In some cases there will be an obvious or natural candidate for a point estimator of a particular parameter.</li>
<li>For example, the sample mean is a good point estimator of the population mean</li>
<li>However, in more complicated models we need a methodical way of estimating parameters.</li>
<li>There are different methods of finding point estimators
<ul>
<li>Method of Moments</li>
<li>Maximum Likelihood Estimators (MLE)</li>
<li>Method of Least Squares <!--
  (Mood page 273) --></li>
<li>Bayes Estimators</li>
<li>The EM Algorithm</li>
</ul></li>
<li>However, these techniques do not carry any guarantees with them</li>
<li>The point estimators that they yeild still must be evaluated before their worth is established</li>
</ul>
<div id="method-of-moments" class="section level4">
<h4><span class="header-section-number">2.2.1.1</span> Method of Moments</h4>
<ul>
<li>Let <span class="math inline">\(X_1, X_2, \dots X_n\)</span> be a random sample from a population with pdf or pmf <span class="math inline">\(f(x; \theta),\)</span> where <span class="math inline">\(\theta = (\theta_1, \theta_2, \dots, \theta_k)\)</span> and <span class="math inline">\(k\geq 1\)</span>.</li>
<li>Sample moments <span class="math inline">\(m^\prime\)</span> and population moments <span class="math inline">\(\mu^\prime\)</span> are defined as follows</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Sample moment</th>
<th>Population moment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(m_1^\prime= \frac{1}{n}\sum_{i=1}^nX_i\)</span></td>
<td><span class="math inline">\(\mu_1=E(X)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(m_2^\prime= \frac{1}{n}\sum_{i=1}^nX_i^2\)</span></td>
<td><span class="math inline">\(\mu_2=E(X^2)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\dots\)</span></td>
<td><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(m_k^\prime= \frac{1}{n}\sum_{i=1}^nX_i^k\)</span></td>
<td><span class="math inline">\(\mu_k=E(X^k)\)</span></td>
</tr>
</tbody>
</table>
<p>Each <span class="math inline">\(\mu_j^\prime\)</span> is a function <span class="math inline">\(\theta,\)</span> i.e. <span class="math inline">\(\mu_j^\prime= \mu_j^\prime(\theta_1, \theta_2, \dots, \theta_k)\)</span> for <span class="math inline">\(j=1,2,\dots, k.\)</span></p>
<p><strong>Method of Moments Estimators (MME)</strong></p>
<p>We first equate the first <span class="math inline">\(k\)</span> sample moments to the corresponding <span class="math inline">\(k\)</span> population moments,</p>
<p><span class="math display">\[m_1^\prime = \mu_1^\prime,\]</span>
<span class="math display">\[m_2^\prime = \mu_2^\prime,\]</span>
<span class="math display">\[\dots\]</span>
<span class="math display">\[m_k^\prime = \mu_k^\prime,\]</span></p>
<p>Then we solve the resulting systems of simultaneous equations for <span class="math inline">\(\hat{\theta_1}, \hat{\theta_2}, \dots, \hat{\theta_k}\)</span></p>
<p><strong>Remarks on Method of Moments Estimators</strong></p>
<ul>
<li>Very easy to compute</li>
<li>Always give an estimator to start with</li>
<li>Generally consistent (Since sample moments are consistent for population moments)</li>
<li>Not necessarily the best or most efficient estimators</li>
</ul>
</div>
<div id="maximum-likelihood-estimators-mle" class="section level4">
<h4><span class="header-section-number">2.2.1.2</span> Maximum Likelihood Estimators (MLE)</h4>
<p><em>Example</em></p>
<p>The number of orders per day coming to a certain company seems to have a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>.</p>
<p>The number of orders received during 10 randomly selected days are as follows:
<span class="math inline">\(12,14,15,12,13,10,11,15,10,6\)</span></p>
<p>Derive an expression for the <span class="math inline">\(P(X_1=12, \; X_2 = 14, \dots, X_{10} = 10)\)</span> as a function of <span class="math inline">\(\lambda.\)</span></p>
<div style="page-break-after: always;"></div>
<p><strong>Find the joint probability of the data</strong></p>
<!--
$$\text{Joint probability of the data} = P(X_1=12, \; X_2 = 14, \dots, X_{10} = 6)$$
$$=\frac{e^{-\lambda}\lambda^{12}}{12!}\frac{e^{-\lambda}\lambda^{14}}{14!}\dots\frac{e^{-\lambda}\lambda^{6}}{6!}$$
-->
<p><img src="figure/Ch2box2-1.png" width="100%" style="display: block; margin: auto;" /></p>
<div class="figure" style="text-align: center"><span id="fig:joint"></span>
<img src="figure/joint-1.png" alt="Probability of the sample is maximum when $\lambda = 11.8$" width="960" />
<p class="caption">
Figure 2.1: Probability of the sample is maximum when <span class="math inline">\(\lambda = 11.8\)</span>
</p>
</div>
<ul>
<li>When it is viewed as a function of <span class="math inline">\(\lambda\)</span>, it is called the <strong>likelihood function of <span class="math inline">\(\lambda\)</span> for the available data</strong></li>
<li>The likelihood for the data is maximum when <span class="math inline">\(\lambda = 11.8.\)</span></li>
<li>Since these data have already occurred, it is very likely that the data have arisen from a Poisson distribution with <span class="math inline">\(\lambda =11.8\)</span>.</li>
<li><p>This estimate for <span class="math inline">\(\lambda\)</span> is called the <strong>maximum likelihood estimate</strong></p></li>
<li><p>In order to define maximum-likelihood estimators, we shall first define the likelihood function.</p></li>
</ul>
<p><strong>Definition: Likelihood function</strong></p>
<p>Let <span class="math inline">\(x_1,x_2, \dots, x_n\)</span> be a set of observations of random variables <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> with the joint density of <span class="math inline">\(n\)</span> random variables, say <span class="math inline">\(f_{X_1, X_2, \dots, X_n}(x_1,x_2, \dots, x_n;\; \theta)\)</span>. This joint density function, which is considered to be a function of <span class="math inline">\(\theta\)</span> is called the <strong>likelihood function of <span class="math inline">\(\theta\)</span> for the set of observations (sample) <span class="math inline">\(x_1,x_2, \dots, x_n.\)</span></strong></p>
<p>In particular, if <span class="math inline">\(x_1,x_2, \dots, x_n\)</span> is a random sample from the density <span class="math inline">\(f(x; \theta)\)</span>, then the likelihood function is <span class="math inline">\(f(x_1; \theta)f(x_2; \theta) \dots f(x_n; \theta).\)</span></p>
<p><em>Notation</em></p>
<p>We use the notation <span class="math inline">\(L(\theta;\;x_1,x_2, \dots, x_n)\)</span> for the likelihood function, in order to remind ourselves to think of the likelihood function as a function of <span class="math inline">\(\theta.\)</span></p>
<ul>
<li>Likelihood function is seen as a function of <span class="math inline">\(\theta\)</span> rather than <span class="math inline">\(x\)</span></li>
<li>Likelihood can be viewed as the degree of plausibility.</li>
<li>An estimate of <span class="math inline">\(\theta\)</span> may be obtained by choosing the most plausible value, i.e., where the likelihood function is maximized.</li>
</ul>
<p><strong>Definition: Maximum Likelihood Estimator</strong></p>
<p>Let <span class="math inline">\(L(\theta)=L(\theta;\;x_1,x_2, \dots, x_n)\)</span> be the likelihood function of <span class="math inline">\(\theta\)</span> for the sample <span class="math inline">\(x_1,x_2, \dots, x_n.\)</span> Suppose <span class="math inline">\(L(\theta)\)</span> has its maximum when <span class="math inline">\(\theta = \hat{\theta}.\)</span></p>
<p>Then <span class="math inline">\(\hat{\theta}\)</span> is called the <strong>Maximum likelihood estimate of</strong> <span class="math inline">\(\theta\)</span>.</p>
<p>The corresponding estimator is called the <strong>Maximum likelihood estimator of</strong> <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li>Many likelihood functions satisfy regularity conditions; so the maximum likelihood estimator is the solution of the equation <span class="math display">\[\frac{dL(\theta)}{d\theta} = 0\]</span></li>
</ul>
<p><strong>Log-likelihood function</strong></p>
<p>Let <span class="math display">\[l(\theta) = ln[L(\theta)].\]</span></p>
<p>Then, <span class="math inline">\(l(\theta)\)</span> is called the <strong>log-likelihood function</strong>.</p>
<ul>
<li>Both <span class="math inline">\(L(\theta)\)</span> and <span class="math inline">\(l(\theta)\)</span> have their maxima at the same value of <span class="math inline">\(\theta\)</span>.</li>
<li>It is sometimes easier tot find the maximum of the logarithm of the likelihood and thereby simplify the calculations in finding the maximum likelihood estimate.</li>
</ul>
<p><strong>Invariance Property of MLE’s</strong></p>
<p>If <span class="math inline">\(\hat{\theta}\)</span> is the MLE of <span class="math inline">\(\theta\)</span>, then for any function <span class="math inline">\(\tau(\theta),\)</span> the MLE of <span class="math inline">\(\tau(\theta)\)</span> is <span class="math inline">\(\tau(\hat{\theta}).\)</span></p>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="desirable-properties-of-point-estimators" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Desirable properties of point estimators</h3>
<ul>
<li><p>We discussed several methods of obtaining point estimators.</p></li>
<li><p>It is possible that different methods of finding estimators will lead to same estimator or different estimators.</p></li>
<li><p>In this section we discuss certain properties, which an estimator may or may not posses, that will guide us in deciding whether one estimator is better than another.</p></li>
</ul>
<div id="unbiasedness" class="section level4">
<h4><span class="header-section-number">2.2.2.1</span> Unbiasedness</h4>
<p><strong>Definition: Unbiased estimator</strong></p>
<p>An estimator <span class="math inline">\(\hat{\theta}\)</span> <span class="math inline">\((=t(X_1, X_2, \dots, X_n))\)</span> is defined to be an <strong>unbiased estimator</strong> of <span class="math inline">\(\theta\)</span> if and only if
<span class="math display">\[E(\hat{\theta}) = \theta\]</span></p>
<ul>
<li><p>The difference <span class="math inline">\(E(\hat{\theta}) -\theta\)</span> is called as the bias of <span class="math inline">\(\hat{\theta}\)</span> and denoted by
<span class="math display">\[Bias(\hat{\theta}) = E(\hat{\theta}) = \theta \]</span></p></li>
<li><p>An estimator whose bias is equal to 0 is called <strong>unbiased</strong>.</p></li>
</ul>
</div>
</div>
<div id="consistency" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Consistency</h3>
<p><strong>Mean-Squared Error</strong></p>
<ul>
<li>The <em>mean-squared error</em> is a measure of goodness or closeness of an estimator to the target.</li>
</ul>
<p><strong>Definition: Mean-squared Error (MSE)</strong></p>
<p>The <strong>mean-squared error</strong> of an estimator <span class="math inline">\(\hat{\theta}\)</span> of <span class="math inline">\(\theta\)</span> is defined as
<span class="math display">\[MSE(\hat{\theta})= E\left[(\hat{\theta} - \theta)^2 \right]\]</span></p>
<ul>
<li><p>The MSE measures the average squared difference between <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>The MSE is a function of <span class="math inline">\(\theta\)</span> and has the interpretation</p></li>
</ul>
<p><span class="math display">\[MSE(\hat{\theta})= Var(\hat{\theta})+ \left[Bias(\hat{\theta}) \right]^2\]</span></p>
<ul>
<li><p>Therefore the MSE incorporates two components, one measuring the variability of the estimator (<em>precision</em>) and the other measuring its bias (<em>accuracy</em>).</p></li>
<li><p>Small value of MSE implies small combined variance and bias.</p></li>
<li><p>If <span class="math inline">\(\hat{\theta}\)</span> is unbiased, then
<span class="math display">\[MSE(\hat{\theta})= Var(\hat{\theta})\]</span></p></li>
<li><p>The positive square root of MSE is known as the <em>root mean squared error</em>
<span class="math display">\[RMSE(\hat{\theta})= \sqrt{MSE(\hat{\theta})}\]</span></p></li>
</ul>
<p><strong>Consistency</strong></p>
<ul>
<li>Estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be consistent for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(MSE(\hat{\theta})\)</span> approaches zero as the sample size <span class="math inline">\(n\)</span> approaches <span class="math inline">\(\infty\)</span>.</li>
</ul>
<p><span class="math display">\[lim_{n\rightarrow \infty}E\left[(\hat{\theta} - \theta)^2 \right]=0\]</span></p>
<ul>
<li>Mean-squared error consistency implies that the bias and the variance both approach to zero as <span class="math inline">\(n\)</span> approaches <span class="math inline">\(\infty.\)</span></li>
</ul>
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="interval-estimation" class="section level2">
<h2><span class="header-section-number">2.3</span> Interval Estimation</h2>
<ul>
<li><p>Under point estimation of a parameter <span class="math inline">\(\theta\)</span>, the inference is a guess of a <strong>single value</strong> as the value of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Instead of making the inference of estimating the true value of the parameter to be a point, under interval estimation we make the inference of estimating that the true value of the parameter is contained in <strong>some interval.</strong></p></li>
</ul>
<div id="what-is-gained-by-using-an-interval-estimator" class="section level3">
<h3><span class="header-section-number">2.3.1</span> What is gained by using an Interval Estimator?</h3>
<p><strong>Example</strong>
<!-- Cassela and berger page 418, mood pg 372--></p>
<ul>
<li>For a sample <span class="math inline">\(X_1, X_2, X_3, X_4\)</span> from a <span class="math inline">\(N(\mu, 1),\)</span> an interval estimator of <span class="math inline">\(\mu\)</span> is <span class="math inline">\([\bar{X}-1, \bar{X}+1].\)</span></li>
<li>This means that we will assert that <span class="math inline">\(\mu\)</span> is in this interval.</li>
<li>In the previous section (Point estimation) we estimated <span class="math inline">\(\mu\)</span> with <span class="math inline">\(\bar{X}.\)</span></li>
<li><p>But now we have the less precise estimator <span class="math inline">\([\bar{X}-1, \bar{X}+1]\)</span>.</p></li>
<li><p>Under interval estimation, by giving up some precision in our estimate (or assertion about <span class="math inline">\(\mu\)</span>), we try to gain some confidence , or assurance that our assertion is correct.</p></li>
</ul>
<p><strong>Explanation</strong></p>
<ul>
<li><p>When we estimate <span class="math inline">\(\mu\)</span> by <span class="math inline">\(\bar{X},\)</span> the probability that the estimator exactly equaled the value of the parameter being estimated is zero (Why? the probability that a continuous random variable equals any value is 0), <em>i.e.</em> <span class="math inline">\(P(\bar{X}=\mu) = 0.\)</span></p></li>
<li><p>However, with an interval estimator, we have a positive probability of being correct.</p></li>
<li><p>The probability that <span class="math inline">\(\mu\)</span> is covered by the interval <span class="math inline">\([\bar{X}-1, \bar{X}+1]\)</span> can be calculated as
<span class="math display">\[P(\mu \in [\bar{X}-1, \bar{X}+1])= P(\bar{X}-1 \leq \mu\leq \bar{X}+1)\]</span>
<span class="math display">\[ = P(-1 \leq \bar{X} - \mu\leq 1)\]</span>
<span class="math display">\[ = P(- 2\leq \frac{\bar{X} - \mu}{\sqrt{1/4}}\leq 2)\]</span>
<span class="math display">\[ = P(- 2\leq Z\leq 2)\;\;\; \left(\frac{\bar{X} - \mu}{\sqrt{1/4}} \text{ is standard normal }\right)\]</span>
<span class="math display">\[=0.9544.\]</span></p></li>
<li><p>Therefore now we have over 95% chance of covering the unknown parameter with the interval estimator.</p></li>
<li><p>By moving for a point to an interval we have scarified some precision in our estimate. But it has resulted in increased confidence that our assertion is correct.</p></li>
<li><p>The purpose of using an interval estimator rather than a point estimator is to have some guarantee of capturing the parameter of interest.</p></li>
</ul>
<!--mood 377, bsc note-->
</div>
<div id="definition-of-confidence-interval" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Definition of confidence interval</h3>
<p><strong>Definition</strong></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from a distribution with parameter <span class="math inline">\(\theta.\)</span> Let <span class="math inline">\(T_1 = g(X_1, X_2, \dots, X_n),\)</span> and <span class="math inline">\(T_2 = h(X_1, X_2, \dots, X_n)\)</span> be two statistics satisfying <span class="math inline">\(T_1 \leq T_2\)</span> for which <span class="math inline">\(P(T_1 &lt; \theta &lt; T_2)= \gamma,\)</span> where <span class="math inline">\(\gamma\)</span> does not depend on <span class="math inline">\(\theta\)</span>. Then, the random interval <span class="math inline">\((T_1, T_2)\)</span> is called a <span class="math inline">\(100\gamma\)</span> <strong>percent confidence interval for</strong> <span class="math inline">\(\theta\)</span>; <span class="math inline">\(\gamma\)</span> is called the confidence coefficient; and <span class="math inline">\(T_1\)</span> <span class="math inline">\(T_2\)</span> are called the lower and upper confidence limits, respectively, for <span class="math inline">\(\theta.\)</span></p>
<p>Suppose that <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> is a realization of <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> and let <span class="math inline">\(t_1 = g(x_1, x_2, \dots, x_n),\)</span> and <span class="math inline">\(t_2 = h(x_1, x_2, \dots, x_n)\)</span>. Then the <em>numerical</em> interval <span class="math inline">\((t_1, t_2)\)</span> is also called a <span class="math inline">\(100\gamma\)</span> <strong>percent confidence interval for</strong> <span class="math inline">\(\theta\)</span>.</p>
<!--* discuss Example 16*-->
</div>
<div id="interpretation-of-confidence-intervals" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Interpretation of confidence intervals</h3>
<ul>
<li><p>Consider the probability statement <span class="math inline">\(P(\bar{X}-1.18 \leq \mu\leq \bar{X}+1.18) =0.95.\)</span></p></li>
<li><p>The above probability statement implies that the random interval <span class="math inline">\((\bar{X}-1.18, \bar{X}+1.18)\)</span> includes the unknown true mean <span class="math inline">\(\mu\)</span> with probability 0.95.</p></li>
</ul>
<div style="page-break-after: always;"></div>
</div>
<div id="methods-of-finding-interval-estimators" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Methods of finding interval estimators</h3>
</div>
<div id="methods-of-evaluating-interval-estimators" class="section level3">
<h3><span class="header-section-number">2.3.5</span> Methods of evaluating interval estimators</h3>

</div>
</div>
<div id="tutorial-1" class="section level2 unnumbered">
<h2>Tutorial</h2>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(X_1, X_2, \dots, X_n \sim iid\;\; N(\mu, \sigma^2),\)</span> both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> unknown. Derive a method of moment estimators for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X_1, X_2, \dots, X_n \sim iid\;\; Bin(n,\theta),\)</span> both <span class="math inline">\(n\)</span> and <span class="math inline">\(\theta\)</span> unknown. Derive a method of moment estimators for <span class="math inline">\(n\)</span> and <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X_1, X_2, \dots, X_n \sim iid\;\; Unif(\theta_1,\theta_2),\)</span> where <span class="math inline">\(\theta_1&lt;\theta_2\)</span>, both unknown. Derive a method of moment estimators for <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X_1, X_2, \dots, X_n \sim Poisson(\lambda).\)</span> Derive a method of moment estimators for <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>Let <span class="math inline">\(X_1, X_2, \dots, X_n \sim iid\;\; Gamma(\alpha,\beta),\)</span> both <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> unknown. Derive a method of moment estimators for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p></li>
</ol>
<p>The survival time (in weeks) of 20 randomly selected male mouse exposed to 240 units of certain type of radiation are given below.</p>
<p><span class="math inline">\(152, 115, 109, 94, 88, 137, 152, 77, 160, 165, 125, 40, 128, 123, 136, 101, 62, 153, 83, 69\)</span></p>
<p>It is believed that the survival times have a gamma distribution. Estimate the corresponding parameters.</p>
<ol start="6" style="list-style-type: decimal">
<li>Let <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> be <span class="math inline">\(n\)</span> random measurements of random variable <span class="math inline">\(X\)</span> with the density function
<span class="math display">\[f_X(x;\lambda)= \lambda x^{\lambda-1},\;\; 0&lt;x&lt;1, \;\; \lambda&gt;0\]</span>
Derive a method of moment estimator for <span class="math inline">\(\lambda\)</span>.</li>
</ol>
<!-- Lecture note example 2.9-->
<ol start="7" style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>. Derive the maximum likelihood estimator of <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>Let <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Derive the maximum likelihood estimators of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p></li>
</ol>
<!-- Lecture note page 58 PhD print note-->
<ol start="9" style="list-style-type: decimal">
<li>Let <span class="math inline">\(X_1, X_2, \dots, X_n \sim iid \; Poisson (\lambda)\)</span>. Find the MLE of <span class="math inline">\(P(X\leq 1)\)</span></li>
</ol>
<!-- Lecture note page 58 PhD print note-->
<ol start="10" style="list-style-type: decimal">
<li>Let <span class="math inline">\(X_1, X_2, \dots, X_n \sim iid \; N (\mu, \sigma^2)\)</span>. Find the MLE of <span class="math inline">\(\mu/\sigma\)</span>.</li>
</ol>
<!-- Lecture note bsc print example 2.3-->
<ol start="11" style="list-style-type: decimal">
<li>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from an exponential distribution with the density function
<span class="math display">\[f_X(x;\lambda)= \lambda e^{-\lambda x},\;\; x&gt;0\]</span>. Is the maximum likelihood estimators of <span class="math inline">\(\lambda\)</span> unbiased?</li>
</ol>
<!-- Lecture note bsc print example 2.1-->
<ol start="12" style="list-style-type: decimal">
<li>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from a distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Show that <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^2 = \frac{\sum_{i=1}^n(X_i-\bar{X})^2}{n-1}\)</span> are unbiased estimators of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, respectively.</li>
</ol>
<!-- Lecture note page 63 PhD print note-->
<ol start="13" style="list-style-type: decimal">
<li>Let <span class="math inline">\(x_1, x_2, \dots, x_n\)</span> be a random sample of size <span class="math inline">\(n\)</span> from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Consider the maximum likelihood estimators of <span class="math inline">\(\sigma^2\)</span>. Show the estimator <span class="math inline">\(\hat{\sigma^2} = \frac{\sum_{i=1}^n(X_i-\bar{X})^2}{n}\)</span> is biased for <span class="math inline">\(\sigma^2\)</span>, but it has a smaller MSE than <span class="math inline">\(S^2 = \frac{\sum_{i=1}^n(X_i-\bar{X})^2}{n-1}\)</span>.</li>
</ol>
<!-- Lecture note page 63 PhD print note-->
<ol start="14" style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from some distribution and <span class="math inline">\(E(X) = \mu\)</span>. Show that <span class="math inline">\(\bar{X}\)</span> is a better estimator than <span class="math inline">\(X_1\)</span> and <span class="math inline">\(\frac{X_1+X_2}{2}\)</span> for <span class="math inline">\(\mu\)</span> in terms of MSE.</p></li>
<li><p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from a <span class="math inline">\(N(\mu, \sigma^2).\)</span> Let <span class="math inline">\(\bar{X}= \frac{\sum_{i=1}^nX_i}{n}\)</span> and <span class="math inline">\(T=\frac{\sum_{i=1}^n(X_i-\bar{X})^2}{n}\)</span>. Show that <span class="math inline">\(\bar{X}\)</span> is consistent for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(T\)</span> is consistent for <span class="math inline">\(\sigma^2.\)</span></p></li>
<li><p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from a <span class="math inline">\(N(\mu,9).\)</span> Find a 95% confidence interval for <span class="math inline">\(\mu\)</span>.</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hypothesis-testing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-Estimation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
